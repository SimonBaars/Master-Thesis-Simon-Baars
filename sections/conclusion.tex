\chapter{Conclusion} \label{ch:conclusion}
We defined automatically refactorable clones and created a tool to detect and refactor them. We measured statistical data with this tool over a large corpus of open-source Java software systems to get more information about the context of clones and how refactoring them influences system maintainability.

%For cloned fragments to be refactorable, we propose that fully qualified identifiers of method call signatures and type references should be considered instead of their plain text representation, to be sure that they refer to the same implementations. , we propose that one should define thresholds for variability in variables, literals and method calls, in order to limit the number of parameters that the merged unit shall have.

To determine which refactoring techniques are most suitable to refactor most clones, we analyzed their context. We measured the \textbf{relation} through inheritance of clone instances in a clone class. In our corpus, we found that 37\% of clones are found in the same class. 24\% of clones are in the same inheritance hierarchy. Another 24\% of clones are unrelated. The final 7\% of clones have the same interface. We also looked at the \textbf{location} of clones: 78\% of clones is found at method-level of which 77\% is found in the body of a method or constructor. Because of this, the ``Extract Method'' refactoring method is most suitable to refactor most clones.

We built a tool that can automatically apply refactorings to 28\% of clones in our corpus using the ``Extract Method'' refactoring technique. The main reason our tool could not refactor all clones is that many clones span certain statements that obstruct method extraction, for instance when code outside the method body is part of a clone.

We measured the effect of clone refactoring on the volume, cyclomatic complexity, number of parameters and duplication metrics of the codebase. We measured these metrics before- and after each clone class that was refactored to determine the impact of each refactoring on system maintainability. We found that the most prominent factor influencing maintainability is the size of the clone. For our dataset, at a clone volume of 29 tokens per clone instance the average clone refactoring improved system maintainability. Another factor with a major impact on maintainability is the number of parameters that the extracted method requires to get all required data. We noticed that the inheritance relation of the clone and the return value of the extracted method has only a minor impact on system maintainability.

% We recommend to extend our tool, CloneRefactor, to allow for such capabilities. CloneRefactor already contains a component that calculates the Cyclomatic Complexity of a given method. Using our automated refactored model, identified problems can relatively easily be refactored.

\section{Threats to validity}\label{chap:threatstovalidity}
In this section we share some factors that can influence the validity of the obtained results.

\subsection{CloneRefactor}
CloneRefactor can refactor all clones that are flagged ``can be extracted''. This is done through a series of AST transformations as described in Section \ref{sec:applyingrefactorings}. Currently, these refactoring methods do not yet allow for variability (type 2R and 3R), as allowing variability quickly increases the complexity of the refactoring problem and the amount of edge cases to consider \cite{tsantalis2015assessing}. Because of this, CloneRefactor does not yet realistically emulate human refactoring efforts, as humans would take variability between code fragments into account. This influences the results, as the numbers obtained might be higher if it would more realistically emulate human behavior.

\subsection{Corpus}
We used a corpus of 2,267 open source Java projects from GitHub for our experiments. This corpus consists of only Maven projects and we only take the ``production sources'' folder of these projects take into account. We inspected the results of CloneRefactor for outliers and excluded projects that have generated sources in their ``production sources'' folder. This increases the chance that we are not taking into account test sources and generated files, but it does not eliminate this chance. Because of that, there is a threat to validity in the source code in the corpus, as it potentially includes sources that do not reflect human-maintained production code.

\section{Future work} \label{sec:future_work}
This study presents a foundation for research in a largely unexplored field of studies: analyzing maintainability through automated refactoring. However, we scratched just the tip of the iceberg regarding all research opportunities in this field. In this section we describe possible extensions to this research, as well as other research opportunities in this field of studies.

\subsection{Automated Refactoring for more metrics}
In this study we presented evidence regarding the value of applying automated refactoring to analyze the before- and after state of source code and refactored source code. Analyzing these states we were able to analyze the improvement in maintainability after refactoring duplicated code. This allows us to better assess thresholds by which maintainability issues in source code are identified. We also get better insights in the costs and values of applying certain refactoring efforts.

For this study we chose to focus only on the automated refactoring of duplication in source code. However, software maintainability depends on more factors and can be measured by more metrics. These factors also have opportunities to automate their refactorings. We think that several similarly sized studies can be conducted to automate the refactoring of other maintainability metrics.

A study by Heitlager et al. \cite{heitlager2007practical} presents several metrics by which the maintainability of source code can be assessed. They propose thresholds that indicate issues with these metrics. Many of these metrics have automated refactoring opportunities. In this section we will focus on several of these metrics to outline their opportunities for automated refactoring.

\subsubsection{Long parameter list}
When multiple parameters are used together in a method, there is an implicit dependency between these parameters: the dependency of being required by that method. \textit{If a lot of data hangs around really tight together, they should be made into their own object} \cite{fowler1999refactoring}. Guidelines describe to limit the number of parameters per method to at most 4 \cite{visser2016building}.

If a method has many parameters, we can group strongly related parameters into an object. This can be done automatically, but two things must be considered:
\begin{itemize}
  \item How do we determine whether parameters are strongly related?
  \item At what other places is this data used in unison and should thus use the new abstraction?
\end{itemize}
To determine whether parameters are strongly related we must look into at what places they are used in the codebase. We must then define some threshold that denotes the percentage of usages of these variables in which they are used together. If this threshold exceeds a certain amount, we can group them into an object. We must then trace all places in which they are used together and replace the variables by the newly created abstractions.

\subsubsection{Method complexity}
Method complexity refers to the complexity of the logic in a method body. There are several methods to compute method complexity. The most used complexity metric is (MCCabe) Cyclomatic Complexity \cite{visser2016building}, which refers to the amount of independent paths that can be taken though the source code. Another complexity metric that has recently become fairly popular is Cognitive Complexity \cite{campbell2017cognitive}, which attempts to measure the human perceived complexity. Both indicate an aspect of source code maintainability.

Dealing with method complexity can largely be done by method extraction. We extract a part of a complex method to a new method. This way we split the complexity of the original method into separately testable methods. Also, the methods become easier to read.

Refactoring complex methods can largely be done automatically. Also, many of the results of this study can be reused. To assess an automated refactoring opportunity for complex methods, we should assess which parts of methods can be split to end up with parts of similar complexity. For this, our research can be used to assess whether a given piece of code can be extracted to a new method (see Section~\ref{sec:refactorability}).

\subsubsection{Method size}
Method size has a strong relation, in terms of refactoring, with Cyclomatic Complexity. Although method size is often related to Cyclomatic Complexity, a study by Landman et al. \cite{landman2016empirical} shows that Cyclomatic Complexity is not redundant to method size. However, they do share a similar method of refactoring. The automated refactoring opportunities described in the previous section also apply to method size.

\subsubsection{Combining the metrics}
Combining the automated refactoring models for each of these metrics can result in a model that ultimately provides significant improvement in the ease of writing well-maintainable source code. In this study, we presented a few tradeoffs that are the result of refactoring code clones. For instance, refactoring code clones with a lot of variability can result in long parameter lists. However, if we can combine this with automated refactoring of long parameter lists, this tradeoff can be mitigated. This way, it is possible to work towards an model of automated refactoring that reduces manual refactoring efforts significantly.

% When programming, there are often trade-offs between technical debt and velocity. When a deadline comes near, often software quality in sacrificed to gain velocity \cite{costello1984software, austin2001effects, shah2014global}. Apart from that, a low programmer aptitude can result in low quality code \cite{cheney1984effects}. This is because often forming appropriate abstractions requires time, effort and critical thinking. By introducing these abstractions automatically, this negative impact can (partly) be mitigated.

%When programming, many developers must take decisions between velocity
%Many programmers report frequent decisions in which they decide to take technical debt

\subsection{Type 4 clones}
In Section~\ref{chap:backgroundclonetypes} we introduced the clone types that are used in literature. Of these clone types, we considered type 1, 2 and 3 for refactoring. We chose not to consider type 4 clones because they appear far less in source code and are more difficult to detect and refactor. However, that does not mean that these clones should not be refactored.

We think that there are relevant research opportunities in refactoring type 4 clones. Type 4 clones are functionally equal pieces of code that are implemented through different implementations. Although functionally equal, type 4 clones might still differ in other aspects. For instance, for type 4 clones, one alternative might be easier to maintain. Also, they can differ in performance.

There are interesting research opportunities in automatically choosing the better alternative among type 4 clone instances.

\subsubsection{Proposing new clone type definitions}
We think that current definitions of clone types still lack in the identification of all duplication issues that a developer should invest his/her refactoring efforts in. Current clone detection techniques still result in many false positives and false negatives. By proposing good definitions for code clones that mark the characteristics of harmful anti-patterns, we can create more accurate suggestions to developers.

\subsection{Naming of refactored methods/classes/etc.}
In this study, we took naming of refactored entities out of the scope. We applied automated refactoring to duplication, and gave all generated methods, classes and interfaces generated names. For our purposes that was not much of an issue, because of the validation methods used. We validated our approach using the SIG Maintainability model, which does not take naming quality into account. Because of that, the results of our experiments are in no way dependent on the names we give the generated methods, classes and interfaces.

When using our work with the purpose of refactoring assistance, these names will have to be manually provided. However, recent studies allow assistance in this process by generating a name that matches the body of a declaration. If this can be done in a reliable way, we can apply refactorings without any manual steps required.

A study by Allamanis et al. \cite{allamanis2015suggesting} proposes a machine learning model that can suggest accurate method and class names. This study shows promising results towards generating method and class names on the basis of their body and context. However, the source code of this study in not available, making it harder to apply their findings to generate class and method names for any software project.

In a recent study by Alon et al. \cite{alon2018code2seq} they propose code2seq. Code2seq is a machine learning model that guesses the name of a method given a method body. This model has been trained on a large set of method bodies and names. The model already shows promising results. The source code of code2seq is publicly available, making it possible to embed this model in any application. Although this model is still far from perfect, combining it with our research could already greatly reduce the manual refactoring effort required. The main deficiency of this model lies in that its limited to the method body and does currently not consider anything else in a method body. This method is flawed, because the meaning of method bodies depends heavily on their context. For instance, the body of the method \texttt{get()} in \texttt{ArrayList} does not make any sense in relation to its name without considering its context (class, inheritance hierarchy, etc.).

\subsection{Looking into other languages/paradigms}
In this study we describe duplicate code refactoring opportunities for object-oriented languages. We built a tool to refactor code clones in Java and used it to run our experiments.

Applying our experiments with other programming languages than just Java might result in valuable results. Refactoring opportunities are greatly dependent on coding conventions, which differ per language. Other languages might result in different results, which might result in different insights regarding the resolution of duplication problems found. We prioritized refactoring efforts based on Java, which might differ from the prioritization that can result from running our experiments with other programming languages.

In this study we focused only on the object-oriented paradigm because of the shared concepts. However, the problems that come with duplication in source code also appears in different programming languages. Because of that, more insights could be obtained when looking into automated refactoring opportunities for other paradigms. For instance, it might be valuable to look into the opportunities to reduce duplication in the functional domain. Dealing with code clones in the functional paradigm is pretty much an unexplored field and might hold valuable results.

\subsection{Automatically refactoring code that is duplicated with a library}
One of the clone detection tools we analyzed in this study is Sisyphus \cite{eremondi2017sisyphus}. This clone detection tool searches a codebase for methods that are part of the Java standard libraries. There are definite automated refactoring opportunities here, automatically switching to the library implementation if available.

This doesn't have to be limited to just the Java standard library. Although using a new library for a single cloned library, we can also search the libraries the project are already uses. CloneRefactor is an excellent tool to expand for finding such clones with external libraries and refactoring the source code to use the library implementation if available. This is because CloneRefactor has scripts to gather all dependent libraries for Maven projects. It loads these libraries in order to be able to resolve symbols. If this would be expanded not only to resolve symbols but also to consider the clones between the analyzed project and its dependencies, we could find such refactoring opportunities.
